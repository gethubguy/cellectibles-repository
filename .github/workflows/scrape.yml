name: Scrape Net54 Forum

on:
  workflow_dispatch:
    inputs:
      scraper_type:
        description: 'Scraper type to use'
        required: true
        type: choice
        options:
          - tapatalk
          - html
        default: tapatalk
      forum_id:
        description: 'Specific forum ID to scrape'
        required: true
        type: string
        default: '39'
      thread_limit:
        description: 'Maximum threads to scrape'
        required: false
        type: string
        default: '10'
  
  schedule:
    # Run weekly on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 hours max
    continue-on-error: true  # Don't show as failed on timeout
    permissions:
      contents: write  # Allow pushing commits
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Create directories
      run: |
        mkdir -p data/forums/net54baseball.com
        mkdir -p logs
    
    # Data is stored in git repository, no need for artifacts
    
    - name: Run scraper
      env:
        BASE_URL: https://www.net54baseball.com
        DELAY_SECONDS: 5
        MAX_RETRIES: 3
        TIMEOUT_SECONDS: 30
      run: |
        chmod +x scripts/scraper_with_commits.sh
        if [ "${{ github.event.inputs.scraper_type }}" = "tapatalk" ]; then
          echo "Using Tapatalk API scraper with periodic commits..."
          ./scripts/scraper_with_commits.sh --forum ${{ github.event.inputs.forum_id }} --topic-limit ${{ github.event.inputs.thread_limit }}
        else
          echo "Using HTML scraper..."
          python scripts/scraper.py --forum ${{ github.event.inputs.forum_id }} --thread-limit ${{ github.event.inputs.thread_limit }}
        fi
    
    - name: Show scraping stats
      if: always()
      run: |
        python scripts/scraper.py --stats
    
    - name: Check scraper exit status
      if: always()
      run: |
        if [ -f scraper_exit_code.txt ]; then
          EXIT_CODE=$(cat scraper_exit_code.txt)
          echo "Scraper exit code: $EXIT_CODE"
          if [ "$EXIT_CODE" != "0" ]; then
            if [ "$EXIT_CODE" = "124" ]; then
              echo "::notice::Scraper reached 6-hour time limit - this is normal for large scraping jobs. Data was saved."
            else
              echo "::warning::Scraper exited with error code $EXIT_CODE but data was preserved"
            fi
          fi
        else
          echo "::warning::Could not find scraper exit code file"
        fi
    
    - name: Commit scraped data
      if: always()
      run: |
        git config --global user.name 'GitHub Action'
        git config --global user.email 'action@github.com'
        git add data/forums/net54baseball.com/
        git diff --staged --quiet || git commit -m "Add scraped data from forum ${{ github.event.inputs.forum_id }} [skip ci]"
        git push || echo "No new data to push"
    
    # Scraped data is committed to git, no artifact needed
    
    # Logs can be viewed in GitHub Actions console
    
    - name: Create summary
      if: always()
      run: |
        echo "## Scraping Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        python scripts/scraper.py --stats >> $GITHUB_STEP_SUMMARY